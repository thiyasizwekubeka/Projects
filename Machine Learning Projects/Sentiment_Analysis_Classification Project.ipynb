{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Climate Change Belief Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Re7Ki9-1vHgW",
        "QXzoixLCrBah"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znLZCkTmvHch"
      },
      "source": [
        "# Climate Change Belief Analysis\n",
        "**Team 2 JHB July 2020**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1dyfTZcvHcj"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxmG-18jrpvq"
      },
      "source": [
        "### Background  \n",
        "\n",
        "In a [research article](https://www.barrons.com/articles/two-thirds-of-north-americans-prefer-eco-friendly-brands-study-finds-51578661728) conducted, 19,000 customers from 28 countries where given a poll to find out how individual shopping decisions are changing. Nearly 70% of consumers in the U.S. and Canada find that it is important for a company or brand to be sustainable or eco-friendly. More than a third (40%) of the respondents globally said that they are purpose-driven consumers, who select brands based on how well they align with their personal beliefs.\n",
        "\n",
        "Many companies are built around lessening their environmental impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. They would like to determine how people perceive climate change and whether or not they believe it is a real threat. This would add to their market research efforts in gauging how their product/service may be received.  \n",
        "\n",
        "The goal of this challenge is to build a Classification Machine Learning model that will determine whether a person believes in Climate Change using tweet data. This model will provide insights of public opinion of Climate Change & consumer sentiment to companies looking to market their new or improved products or services to consumers, in response to CER.\n",
        "\n",
        "As the demand for sustainable, eco-friendly products and services by consumers increases, a sentiment classification model that identifies these potential customers is key and could be used any business or organisation committed to carbon neutrality & wanting to inform marketing strategies. This includes, but is not limited to companies in the retail, automotive, government, agriculture & food, pharmaceutical spheres. The model could also be used by sectors in government wanting to identify the various belief sentiments in order to better direct environmental awareness and education campaigns in alignment with their legislative directives and climate change response plans.\n",
        "\n",
        "\n",
        "### Problem statement  \n",
        "\n",
        "Build a machine learning model that is able to classify whether or not an individual believes in man-made climate change based on historical tweet data to increase insights about customers and inform future marketing strategies.\n",
        "\n",
        "You can find the project overview [here](https://www.kaggle.com/c/climate-change-edsa2020-21)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKtHPSg8vHcl"
      },
      "source": [
        "# Notebook outline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blljOeqivHcm"
      },
      "source": [
        "1. Installations and Imports\n",
        "2. Explore Data Analysis\n",
        "\n",
        "**Base Model**\n",
        "3. Data Preprocessing\n",
        "4. Text Feature Extraction\n",
        "5. Model Building\n",
        "6. Model Evaluation\n",
        "7. Model Analysis\n",
        "8. Submition\n",
        "\n",
        "**Tuned and Improved Model**\n",
        "9. Data Preprocessing\n",
        "10. Text Feature Extraction\n",
        "11. Modelling\n",
        "12. Model Performance\n",
        "13. Hyperparameter Tuning of Best Models\n",
        "14. Model Analysis\n",
        "15. ROC Curves and AUC\n",
        "16. Save Output\n",
        "17. Conclusion\n",
        "18. Comet\n",
        "19. References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3FarnDBvHco"
      },
      "source": [
        "# 1. Installations and Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cafBUg5YvHcq"
      },
      "source": [
        "### 1.1 Installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W10-CT50vHcr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a340540-74d0-48b5-a7b5-3f72b22eef32"
      },
      "source": [
        "pip install comet_ml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: comet_ml in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: netifaces>=0.10.7 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (0.10.9)\n",
            "Requirement already satisfied: websocket-client>=0.55.0 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (0.57.0)\n",
            "Requirement already satisfied: dulwich>=0.20.6; python_version >= \"3.0\" in /usr/local/lib/python3.6/dist-packages (from comet_ml) (0.20.11)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from comet_ml) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (1.12.1)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (2.23.0)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (7.352.0)\n",
            "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (2.6.0)\n",
            "Requirement already satisfied: wurlitzer>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (2.0.1)\n",
            "Requirement already satisfied: everett[ini]>=1.0.1; python_version >= \"3.0\" in /usr/local/lib/python3.6/dist-packages (from comet_ml) (1.0.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from dulwich>=0.20.6; python_version >= \"3.0\"->comet_ml) (2020.6.20)\n",
            "Requirement already satisfied: urllib3>=1.24.1 in /usr/local/lib/python3.6/dist-packages (from dulwich>=0.20.6; python_version >= \"3.0\"->comet_ml) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet_ml) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet_ml) (2.10)\n",
            "Requirement already satisfied: configobj; extra == \"ini\" in /usr/local/lib/python3.6/dist-packages (from everett[ini]>=1.0.1; python_version >= \"3.0\"->comet_ml) (5.0.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1isR9t1hvHc3"
      },
      "source": [
        "### 1.2 Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBJjh1MkvHdB"
      },
      "source": [
        "from comet_ml import Experiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrKcOfw5gfZW"
      },
      "source": [
        "# Create an experiment with your api key:\n",
        "experiment = Experiment(\n",
        "    api_key=\"06V8ejxSIh2dFMs9ne4vusQXq\",\n",
        "    project_name=\"climate-change-belief-analysis\",\n",
        "    workspace=\"bmqhamane\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo9ZVnqevHdM"
      },
      "source": [
        "Import python libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfA8EUi5vHdO"
      },
      "source": [
        "\n",
        "# Loading Data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "import time\n",
        "\n",
        "# Data Preprocessing\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.utils import resample\n",
        "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Model Building\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "\n",
        "# Model Evaluation\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
        "#from scikitplot.metrics import plot_roc, plot_confusion_matrix\n",
        "\n",
        "# Explore Data Analysis\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from matplotlib.pyplot import rcParams\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvnQWABfqaqM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b966b0a5-0fac-41bf-c125-7be408935e2f"
      },
      "source": [
        "#download libraries\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "sns.set_style('whitegrid')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQR2y7-SvHdU"
      },
      "source": [
        "### 1.3 Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6oGh1TItPIl",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "outputId": "b28dc05b-eac8-4762-a0df-ab199848bbce"
      },
      "source": [
        "from google.colab import files \n",
        "  \n",
        "  \n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1ae286e1-b011-4ba7-a2f3-589accae8e43\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1ae286e1-b011-4ba7-a2f3-589accae8e43\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrWLDcn4vHdV"
      },
      "source": [
        "We will load our data as a Pandas DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frFngyXDvHdW"
      },
      "source": [
        "train = pd.read_csv('/content/train.csv')\n",
        "test = pd.read_csv('/content/test.csv') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHc4KP78vHey"
      },
      "source": [
        "# 2. Exploratory Data Analysis (EDA)\n",
        "\n",
        "The section is an exploration of the data through an analysis of the different Climate Change sentiments that people have on Twitter.\n",
        "\n",
        "**Techniques that we are going to use to analyse our data**\n",
        "\n",
        "- Understanding the distribution of sentiments\n",
        "- An analysis of the Tweets statistics\n",
        "- Understanding the length of our tweets\n",
        "- The main topics on climate change"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RH-wLi8Lhy2"
      },
      "source": [
        "#create a copy of the origional data\n",
        "ftrain = train.copy()\n",
        "ftest = test.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Naw5xvnDlvbu"
      },
      "source": [
        "print('There are', len(ftrain), 'rows and',ftrain.shape[1], 'columns in the train set.')\n",
        "print('There are', len(ftest), 'rows and',ftest.shape[1], 'columns in the test set.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snmbeBfaqCcs"
      },
      "source": [
        "Checking for null values in the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELQvSUfTLh4b"
      },
      "source": [
        "#test data\n",
        "ftest.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzpaK0qKLiBi"
      },
      "source": [
        "#train data\n",
        "ftrain.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crjjJ0wtL5IK"
      },
      "source": [
        "## 2.1 The distribution of climate change sentiments \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Understanding the distribution of sentiments surrounding climate change on Twitter communicates that there are different views on climate change hence the different classes associated with these views/sentiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFSLQk59L0sW"
      },
      "source": [
        "#extract the value counts per sentiment class\n",
        "a = ftrain.sentiment.value_counts()\n",
        "#calculate the percentage of each sentiment class\n",
        "b = 100*ftrain.sentiment.value_counts()/len(ftrain.sentiment)\n",
        "b = round(b,2)\n",
        "data = pd.concat([a,b],axis =1,)\n",
        "data.columns = ['Value Count', 'Percentage']\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJFOcVY4L0n6"
      },
      "source": [
        "sns.countplot(x='sentiment',data=ftrain,palette='rainbow')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0v0Qz9JiMRXO"
      },
      "source": [
        "As seen in the bar graph, sentiment class 1 has the highest number of tweets in the train data accounting for 8530 tweets(53.92%).The lowest sentiment class is class -1 which accounts for 1296 tweets (8.19%).The distribution of sentiments classes are imbalanced because the classes do not have the same ammount of tweets in their class as seen in dataframe which compares the value counts and percentage of each sentiment class.\n",
        "\n",
        "The class imbalance of the training data has an impact on the classification made on the unseen data (testing data) in the modeling phase.A class imbalance could result in the model classifying most of the tweets into sentiment class 1 since the model gets better a classifying class 1 tweets as the model has more evidence  of class 1 tweets.This will be taken into consideration in the preprocessing and modeling section of the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcfSNOWRMVsp"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "## 2.2 An overview of tweets statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkC5Lb5cL0jr"
      },
      "source": [
        "#brief description of the train data\n",
        "ftrain.message.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oat7-XyUL0gE"
      },
      "source": [
        "#brief description of the test data\n",
        "ftest.message.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaEmiR48L0cF"
      },
      "source": [
        "#description of the data per sentiment class\n",
        "ftrain[['sentiment','message']].groupby('sentiment').describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2x0zq3NOrGrU"
      },
      "source": [
        "Adding a column of the tweets length/character count to the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5-obFETL0Zw"
      },
      "source": [
        "ftrain['length'] = ftrain['message'].apply(len)\n",
        "ftrain.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sn58d8ZMj22"
      },
      "source": [
        "#creating a lenght column\n",
        "ftest['length'] = ftest['message'].apply(len)\n",
        "ftest.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeaCrUK_MuV6"
      },
      "source": [
        "## 2.3 The distribution of the tweets length in the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LckjRtJMkDH"
      },
      "source": [
        "sns.distplot(ftrain['length'],bins=30,kde=False,color='#440154')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0H3yWkIeMkda"
      },
      "source": [
        "ftrain['length'].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riCIBhH1MkYR"
      },
      "source": [
        "#print the longest tweet in the train data\n",
        "ftrain[ftrain['length'] == 208]['message'].iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xbQwZBdM5jv"
      },
      "source": [
        "The tweets length in the train data lie between 208 characters and 14 characters.The average length of tweets is 123 characters.The longest tweet on climate change in the train data contrains 208 words.The longest tweet stands out from the average length of tweets on climate change which is 123 words.The cell illustrates that the tweet with the most words is simply made up of only a few actual words this will be taken into consideration in the preprocessing section of the notebook to ensure that any noise in the tweets are removed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHAyaKVmMkUr"
      },
      "source": [
        "sns.distplot(ftest['length'],bins=30,kde=False,color='#20A387')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aILJAfjHMj_1"
      },
      "source": [
        "ftest['length'].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqnwE_TJMj80"
      },
      "source": [
        "ftest[ftest['length'] == 623]['message'].iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3Vf-XfKNGK4"
      },
      "source": [
        "The tweet in the test data are betweet 7 characters and 623 characters.On average the tweets in the test data are 123 characters.The longest tweets seem to have soe discrepency because twitter's word limit  on tweets in 280 characters however the longest tweet exceeds this limit.The longest tweet in the data is simply made up of only a few actual words this will be taken into consideration in the preprocessing section of the notebook to ensure that any noise in the tweets are removed "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEIGGxtINKFM"
      },
      "source": [
        "### The length of tweets per sentiment class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JMKAJ82NB9M"
      },
      "source": [
        "g = sns.FacetGrid(ftrain,col='sentiment')\n",
        "g.map(plt.hist,'length')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeMBX_-KNVRQ"
      },
      "source": [
        "Tweets that are part of sentiment class one have have the highest length frequency as compared to the other classes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3WfZ3csNZvK"
      },
      "source": [
        "\n",
        "## 2.4 The main topics surrounding the climate change tweets\n",
        "\n",
        "An understanding of the main topics dicussed in the climate change discussion on twitter is essential as it illustrates the sentiments attatched to climate change. This is done through extracting the most frequently used words and hashtags."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgKvHOerNfHS"
      },
      "source": [
        "### 2.4.1 Top 30 used words in the tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtcnID8rNjVD"
      },
      "source": [
        " Train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cM7uTPZNRp3"
      },
      "source": [
        "#convert the test to numerical values \n",
        "cv = CountVectorizer(stop_words = 'english')\n",
        "words = cv.fit_transform(ftrain.message)\n",
        "\n",
        "sum_words = words.sum(axis=0)\n",
        "#create a frequency of most occuring words\n",
        "words_freq = [(word, sum_words[0, i]) for word, i in cv.vocabulary_.items()]\n",
        "words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n",
        "#create a dataframe of the words and frequency \n",
        "frequency = pd.DataFrame(words_freq, columns=['word', 'freq'])\n",
        "\n",
        "frequency.head(30).plot(x='word', y='freq', kind='bar', figsize=(15, 7), color = '#440154')\n",
        "plt.title(\"Train : Most Frequently Occuring Words - Top 30\",size=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pm2ZmdXyNn9q"
      },
      "source": [
        "Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpTmHs65NRwf"
      },
      "source": [
        "cv = CountVectorizer(stop_words = 'english')\n",
        "words = cv.fit_transform(ftest.message)\n",
        "\n",
        "sum_words = words.sum(axis=0)\n",
        "\n",
        "words_freq = [(word, sum_words[0, i]) for word, i in cv.vocabulary_.items()]\n",
        "words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n",
        "\n",
        "frequency = pd.DataFrame(words_freq, columns=['word', 'freq'])\n",
        "\n",
        "frequency.head(30).plot(x='word', y='freq', kind='bar', figsize=(15, 7), color = '#20A387')\n",
        "plt.title(\"Test : Most Frequently Occuring Words - Top 30\", size =15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLMe0OvLNR6v"
      },
      "source": [
        "#creating a word cloud from the data\n",
        "wordcloud = WordCloud(background_color = 'white', \n",
        "                      width = 1000, height = 1000).generate_from_frequencies(dict(words_freq))\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.title(\"WordCloud - Vocabulary from tweets\")\n",
        "plt.imshow(wordcloud)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VFzvzQ6eQuA"
      },
      "source": [
        "### 2.4.2 The top 10 influencial Twitter accounts per Sentiment Class\n",
        "\n",
        "The accounts that recieved the most mentions are Twitter accounts that have engaged with the climate change topic.Twitter users mention these accounts when reposting(retweeting) the twitter accounts sentiment on climate change or responding to the twitter accounts comment on climate change.Within the data these Twitter accounts have played a vital role in fueling the climate change debate on Twitter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ge91Uuqo1zZf"
      },
      "source": [
        "def mentions(text):\n",
        "    \"\"\"\n",
        "    The function extracts all the \n",
        "    mentions from the message columns\n",
        "    \"\"\"\n",
        "    line=re.findall(r'(?<=@)\\w+',text)\n",
        "    return \" \".join(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOlEtU0SPGn4"
      },
      "source": [
        "#creating a mentions column\n",
        "ftrain['mentions']=ftrain['message'].apply(lambda x:mentions(x))\n",
        "\n",
        "train_neg = ftrain.loc[ftrain['sentiment'] == -1]\n",
        "train0 = ftrain.loc[ftrain['sentiment'] == 0]\n",
        "train1 = ftrain.loc[ftrain['sentiment'] == 1]\n",
        "train2 = ftrain.loc[ftrain['sentiment'] == 2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7dCbtw6O2QT"
      },
      "source": [
        "#counting the mentions in the data\n",
        "temp_neg= train_neg['mentions'].value_counts()[:][1:11]\n",
        "\n",
        "temp_neg =temp_neg.to_frame().reset_index().rename(columns={'index':'Mentions','mentions':'count'})\n",
        "\n",
        "plt.figure(figsize=(16,5))\n",
        "x= temp_neg['Mentions']\n",
        "y= temp_neg['count']\n",
        "\n",
        "plt.title('Sentiment Class -1',size =15)\n",
        "sns.barplot(x=y,y=x,color='#ff7f00')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wY0ohjjf1yg1"
      },
      "source": [
        "#counting the mentions in the data\n",
        "temp0= train0['mentions'].value_counts()[:][1:11]\n",
        "temp0 =temp0.to_frame().reset_index().rename(columns={'index':'Mentions','mentions':'count'})\n",
        "plt.figure(figsize=(16,5))\n",
        "\n",
        "x= temp0['Mentions']\n",
        "y= temp0['count']\n",
        "\n",
        "plt.title('Sentiment Class 0',size =15)\n",
        "sns.barplot(x=y,y=x,color='#fb9a99')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xB9jhdTB1xeN"
      },
      "source": [
        "#counting the mentions in the data\n",
        "temp1= train1['mentions'].value_counts()[:][1:11]\n",
        "temp1 =temp1.to_frame().reset_index().rename(columns={'index':'Mentions','mentions':'count'})\n",
        "plt.figure(figsize=(16,5))\n",
        "\n",
        "x= temp1['Mentions']\n",
        "y= temp1['count']\n",
        "plt.title('Sentiment Class 1',size =15)\n",
        "sns.barplot(x=y,y=x,color='#33a02c')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gDOiZEF1v47"
      },
      "source": [
        "#counting the mentions in the data\n",
        "temp2= train2['mentions'].value_counts()[:][1:11]\n",
        "temp2 =temp2.to_frame().reset_index().rename(columns={'index':'Mentions','mentions':'count'})\n",
        "plt.figure(figsize=(16,5))\n",
        "\n",
        "x= temp2['Mentions']\n",
        "y= temp2['count']\n",
        "plt.title('Sentiment Class 2',size =15)\n",
        "sns.barplot(x=y,y=x,color='#b2df8a')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D442La6xN1di"
      },
      "source": [
        "### 2.4.3 An analysis of the Hashtags used  per sentiment class\n",
        "\n",
        "A hashtags is written using the '#' symbol.Its main function is to categorize tweets based on a keyword or a topic associated with the hashtag used. According to the 'Twitter Help Center' website people use hashtags before a relevant phrase or keyword. \n",
        "\n",
        "The hashtags used in the climate change tweets highlight the people's interest in the climate change topic.The hashtags that were used communicate that people have divided opinions on climate change.This is relfected in the hashtags used within each sentiment class.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhXGOu6BNR3g"
      },
      "source": [
        "# collecting the hashtags\n",
        "\n",
        "def hashtag_extract(x):\n",
        "    \"\"\"\n",
        "    The function extract the hashtags\n",
        "    from the messages column\n",
        "    \"\"\"\n",
        "    hashtags = []    \n",
        "    for i in x:\n",
        "        ht = re.findall(r\"#(\\w+)\", i)\n",
        "        hashtags.append(ht)\n",
        "    return hashtags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHFRwe3zNCD7"
      },
      "source": [
        "# extracting hashtags from train tweets\n",
        "HT_train_neg = hashtag_extract(ftrain['message'][ftrain['sentiment'] == -1])\n",
        "HT_train0 = hashtag_extract(ftrain['message'][ftrain['sentiment'] == 0])\n",
        "HT_train1 = hashtag_extract(ftrain['message'][ftrain['sentiment'] == 1])\n",
        "HT_train2 = hashtag_extract(ftrain['message'][ftrain['sentiment'] == 2])\n",
        "\n",
        "\n",
        "# unnesting list\n",
        "HT_train_neg = sum(HT_train_neg,[])\n",
        "HT_train0 = sum(HT_train0,[])\n",
        "HT_train1 = sum(HT_train1,[])\n",
        "HT_train2 = sum(HT_train2,[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnbiXm8lOAiT"
      },
      "source": [
        "#### 2.4.3.1 Top 10 hashtags used in Sentiment class  -1 tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l56NKddoN8Yd"
      },
      "source": [
        "#creating a frequency distribution of the hashtags\n",
        "a = nltk.FreqDist(HT_train_neg)\n",
        "d = pd.DataFrame({'Hashtag': list(a.keys()),\n",
        "                  'Count': list(a.values())})\n",
        "\n",
        "# selecting top 10 most frequent hashtags     \n",
        "d = d.nlargest(columns=\"Count\", n = 10) \n",
        "plt.figure(figsize=(16,5))\n",
        "ax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\", color ='#ff7f00')\n",
        "ax.set(ylabel = 'Count')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc6J9os4N8iM"
      },
      "source": [
        "#An example of a sentiment found within class -1 tweets\n",
        "ftrain[ftrain['sentiment'] == -1]['message'].iloc[67]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_zhTlkdOHtp"
      },
      "source": [
        "In class -1 the hashtag that was used the most is #MAGA and the second highest being #climate.These keywords were the most used when people were discussing their sentiments concerning climate change.Other interesting hashtags that form part of the top ten hashtags used in class one are #fakenews and #ClimateScam which insinuate that some of the people who were tweeting about climate change believe that is is simply fake news or a scam. The third highest hashtag used is #Trump when discussing climate change. The class focuses more on discussing climate change as being linked to politics hence the hashtag that has been used the most is #MAGA as well as the example of one of the tweets provided in the cell above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLBnzveuONDh"
      },
      "source": [
        "#### 2.4.3.2 Top 10 hashtags used in Sentiment class 0 tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMQQ6R1qPhmy"
      },
      "source": [
        "#creating a frequency distribution of the hashtags\n",
        "a = nltk.FreqDist(HT_train0)\n",
        "c = pd.DataFrame({'Hashtag': list(a.keys()),\n",
        "                  'Count': list(a.values())})\n",
        "\n",
        "# selecting top 10 most frequent hashtags \n",
        "c = c.nlargest(columns=\"Count\", n = 10) \n",
        "plt.figure(figsize=(16,5))\n",
        "ax = sns.barplot(data=c, x= \"Hashtag\", y = \"Count\",color ='#fb9a99')\n",
        "ax.set(ylabel = 'Count')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rmEdGQwN8fP"
      },
      "source": [
        "#An example of a sentiment found within class 0 tweets\n",
        "ftrain[ftrain['sentiment'] == 0]['message'].iloc[184]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tq8gAx2NB5x"
      },
      "source": [
        "#An example of a sentiment found within class 0 tweets\n",
        "ftrain[ftrain['sentiment'] == 0]['message'].iloc[197]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24IyPggpOZei"
      },
      "source": [
        "The keyword that is used the most when discussing climate change is #climate followed by #climatechange.#Trump is a prominent hashtag in class 0 as well.Donald Trump's views on climate change is discussed in the class.An interesting hashtag used by people is #BeforeTheFlood which is a movie that depicts the impacts of climate change on the Earth,as well as #amreading people use this hashtage to tell mention what they a book or article they are currently reading. The sentiments within class 0 are open conversations surrounding climate change including people asking questions about climate change as well as sarcasm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGhpQ2IhOdTL"
      },
      "source": [
        "#### 2.4.3.3 Top 10 hashtags used in Sentiment class 1 tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDRPW38aOWUo"
      },
      "source": [
        "#creating a frequency distribution of the hashtags\n",
        "a = nltk.FreqDist(HT_train1)\n",
        "d = pd.DataFrame({'Hashtag': list(a.keys()),\n",
        "                  'Count': list(a.values())})\n",
        "\n",
        "# selecting top 10 most frequent hashtags     \n",
        "d = d.nlargest(columns=\"Count\", n = 10) \n",
        "plt.figure(figsize=(16,5))\n",
        "ax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\",color ='#33a02c')\n",
        "ax.set(ylabel = 'Count')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41IDGXabOWRu"
      },
      "source": [
        "#An example of a sentiment found within class 1 tweets\n",
        "ftrain[ftrain['sentiment'] == 1]['message'].iloc[89]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBTaTbjNOw7l"
      },
      "source": [
        "The opinions on climate change in class 1 shift towards climate change does exist as the conversations in this class discuss a movie called Before the flood.The movie highlights the impact of climate change on the Earth.As well as using the hashtag  #ActOnClimate, the tweets associated with the hastag on Twitter mainly discuss ways to combat climate change (http://www.tweepy.net/hashtag/ActOnClimate). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB0Q2kAeO6fz"
      },
      "source": [
        "#### 2.4.3.4 Top 10 hashtags used in Sentiment class 2 tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "934Ic_fyLh-0"
      },
      "source": [
        "#creating a frequency distribution of the hashtags\n",
        "a = nltk.FreqDist(HT_train2)\n",
        "d = pd.DataFrame({'Hashtag': list(a.keys()),\n",
        "                  'Count': list(a.values())})\n",
        "\n",
        "# selecting top 10 most frequent hashtags     \n",
        "d = d.nlargest(columns=\"Count\", n = 10) \n",
        "plt.figure(figsize=(16,5))\n",
        "ax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\",color='#b2df8a')\n",
        "ax.set(ylabel = 'Count')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvxmuM3YO9qm"
      },
      "source": [
        "#An example of a sentiment found within class 2 tweets\n",
        "ftrain[ftrain['sentiment'] == 2]['message'].iloc[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFmbsYdQPIzq"
      },
      "source": [
        "The opinions in class one mainly focus on the climate this is evident in the high hashtag count of the word #climate, the second highest is #enviroment .The class is mainly focused on informing people about climate change and its effect on the enviroment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUhf-Ot_PS41"
      },
      "source": [
        "# 2.5 The key findings from the EDA\n",
        "\n",
        "* There are polarised views on climate change on twitter\n",
        "\n",
        "* Within the data there exists a class imbalance,this will be considered in the preprocessing and model training section\n",
        "\n",
        "* An analysis of the hashtags has shown that the tweets in class 1 believe in climate change,class 2 believe and inform people about climate change,class 0 are more neutral and tend to downplay the existence of climate change and class -1 do not believe that climate change exists.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Bqz5LRB4veN"
      },
      "source": [
        "#class_1 the PRO class\n",
        "#class 2 the NEWS class\n",
        "#class 0 NUETRAL class\n",
        "#class -1 the ANTI class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFXRPK2sBuZH"
      },
      "source": [
        "# The Based Model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rA64sGcB3kF"
      },
      "source": [
        "In this section, we will be cover the process of building a base model starting from the preprocessing of data up to the model building and evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xerdHT_VvHdd"
      },
      "source": [
        "# 3. Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Jnqarig2sF1"
      },
      "source": [
        "Preprocessing involves the elimination of trivial or less informative data, which does not contribute to the sentiment classification. To understand the process of eliminating less informed data, it is important to understand what matters in sentiment analysis. Words are the most important part, however, when it comes to things like punctuation, you cannot get the sentiment from punctuation. Therefore, punctuation does not matter in sentiment analysis. In addition, tweet elements such as images, videos, URLs, usernames, emojis do not contribute to the polarity of the tweet (whether positive or negative). However, this is only true for machine learning models.\n",
        "\n",
        "**Techniques that we are going to use to clean our data**\n",
        "\n",
        "- Removing Noise\n",
        "- Stop Words\n",
        "- Tokenisation\n",
        "- Lemmatisation Normalization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq16ZyIBzwUK"
      },
      "source": [
        "###3.1 Dealing with Class Imbalance - Resampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBsButXczz2K"
      },
      "source": [
        "The EDA highlighted that there is a class imbalance within the data.In training classification model, it is preferable for all classes to have a relatively even split of observations. However, in the wild, classification datasets often come with unevenly distributed observations with one class or set of classes having way more observations than others.This will negatively affecting the accuracy score of the model. Therefore resampling is necessary before training a model with this data.\n",
        "\n",
        "Resampling methods aim at modifying the dataset in order to reduce the discrepancy among the sizes of the classes. In this regard, two scenarios are proposed: one that eliminates instances from the majority class - called undersampling, and one that generates instances for the minority class - called over-sampling. They both have there pros and cons.In other words, Both oversampling and undersampling involve introducing a bias to select more samples from one class than from another, to compensate for an imbalance that is either already present in the data, or likely to develop if a purely random sample were taken. Pykes mentined that \"the random oversampling may increase the likelihood of overfitting occurring since it makes exact copies of the minority class examples. In this way, a symbolic classifier, for instance, might construct rules that are apparently accurate, but actually cove one replicated example\" and “In random under-sampling (potentially), vast quantities of data are discarded. This can be highly problematic, as the loss of such data can make the decision boundary between the minority and majority instances harder to learn, resulting in a loss in classification performance.”"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOfsLFt-1XI4"
      },
      "source": [
        "from IPython.display import Image\n",
        "Image('resampling.png', width=\"800\" ,height=\"400\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Xt62eZ5zan2"
      },
      "source": [
        "#### Combining Both Random Sampling Techniques\n",
        "\n",
        "Combining both random sampling methods can occasionally result in overall improved performance in comparison to the methods being performed in isolation. In this predict we will balance our data by using both methods oversampling and undersampling method. The class size is determined by the average of data points. If a class is less than the average the class will be upsampled and if the class is greater than the average, then the class will be downsampled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYcv4I4F1-JK"
      },
      "source": [
        "def resambling(df):\n",
        "    \"\"\"\n",
        "        The functions takes in dataframe and resample the classses base on class size.\n",
        "        The class size is a average of the datasets among the classes.\n",
        "        This function resamples by downsampling classes with observations greater than the class size and\n",
        "        upsampling the classes with observations smaller than the class size.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    class_2 = df[df['sentiment'] == 2]  \n",
        "    class_1 = df[df['sentiment'] == 1]  \n",
        "    class_0 = df[df['sentiment'] == 0]  \n",
        "    class_n1 = df[df['sentiment'] == -1] \n",
        "    class_size = int((len(class_1)+len(class_2)+len(class_0)+len(class_n1))/4)\n",
        "    # Downsampling class_1 the PRO class\n",
        "    rclass_1 = resample(class_1, replace=True, n_samples=class_size, random_state=42)\n",
        "    #upsampling class 2 the NEWS class\n",
        "    rclass_2 = resample(class_2, replace=True, n_samples=class_size, random_state=42)\n",
        "    #upsampling class 0 NUETRAL class\n",
        "    rclass_0 = resample(class_0, replace=True, n_samples=class_size, random_state=42)\n",
        "    #upsampling class -1 the ANTI class\n",
        "    rclass_n1 = resample(class_n1, replace=True, n_samples=class_size, random_state=42)\n",
        "    sampled_df = pd.concat([rclass_2, rclass_1, rclass_0, rclass_n1])\n",
        "    \n",
        "    return sampled_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAVI62kV2Cwb"
      },
      "source": [
        "Resampled_train_df = resambling(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3NWdLVk2UPO"
      },
      "source": [
        "news=Resampled_train_df[Resampled_train_df.sentiment == 2].shape[0]\n",
        "pro =Resampled_train_df[Resampled_train_df.sentiment == 1].shape[0]\n",
        "neutral=Resampled_train_df[Resampled_train_df.sentiment == 0].shape[0]\n",
        "anti =Resampled_train_df[Resampled_train_df.sentiment == -1].shape[0]\n",
        "#visualising\n",
        "plt.figure(1,figsize=(14,8))\n",
        "plt.bar([\"News\", \"Pro\", \"Neutral\" , \"Anti\"],[news, pro, neutral , anti])\n",
        "plt.xlabel('Tweet_class')\n",
        "plt.ylabel('Sentiment counts')\n",
        "plt.title('Class Distributions')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKjsH0rT2egv"
      },
      "source": [
        "## 3.2 Text Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rg-g1_Sl3Bh4"
      },
      "source": [
        "Before we begin with data cleaning we created copies of the dataframe which allows us to make some changes without changing the original dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruYPcQf73JBl"
      },
      "source": [
        "# Creating copies of dataframes\n",
        "train_copy = Resampled_train_df.copy()\n",
        "test_copy = ftest.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEoxaWkX3N6Z"
      },
      "source": [
        "### 3.2.1 Removing Noise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlMOeIN03TeB"
      },
      "source": [
        "In text analysis, eliminating noise  is the most important part of getting the data into usable format. \n",
        "\n",
        "We will remove noise with the following steps.\n",
        "- Convert letters to lowercases\n",
        "- Remove URL links \n",
        "- Remove hashtag/numbers\n",
        "- Remove punctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRvQUpWO3bCV"
      },
      "source": [
        "def cleaner(tweet):\n",
        "    \"\"\"\n",
        "    this function takes in a dataframe and perform the following:\n",
        "    -Convert letters to lowercases\n",
        "    -remove URL links\n",
        "    -remove # from hashtags\n",
        "    -remove numbers\n",
        "    -remove punctuation\n",
        "    from the text field then return a clean dataframe \n",
        "    \"\"\"\n",
        "    tweet = tweet.lower()\n",
        "    to_del = [\n",
        "        r\"@[\\w]*\",  # strip account mentions\n",
        "        r\"http(s?):\\/\\/.*\\/\\w*\",  # strip URLs\n",
        "        r\"#\\w*\",  # strip hashtags\n",
        "        r\"\\d+\",  # delete numeric values\n",
        "        r\"U+FFFD\",  # remove the \"character note present\" diamond\n",
        "    ]\n",
        "    for key in to_del:\n",
        "        tweet = re.sub(key, \"\", tweet)\n",
        "    \n",
        "    # strip punctuation and special characters\n",
        "    tweet = re.sub(r\"[,.;':@#?!\\&/$]+\\ *\", \" \", tweet)\n",
        "    # strip excess white-space\n",
        "    tweet = re.sub(r\"\\s\\s+\", \" \", tweet)\n",
        "    \n",
        "    return tweet.lstrip(\" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egceRwED3goZ"
      },
      "source": [
        "train_copy['message'] = train_copy['message'].apply(cleaner)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_hcAAD23lzt"
      },
      "source": [
        "train_copy.tail(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apVDSCWY3tLP"
      },
      "source": [
        "### 3.2.2 Removing Stop Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgKzoRKg30pv"
      },
      "source": [
        "The stop words are the most common words like \"if\", \"but\", \"we\", \"he\", \"she\" and \"she\". We can usually remove these words without changing the semantics of any text, and doing so often (but not always) improves the performance of a model. Removing these stop words becomes much more useful when we use longer sequences of words as model features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Njd_PaB535b3"
      },
      "source": [
        "stop_word = stopwords.words('english')\n",
        "train_copy['message'] = train_copy['message'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_word)]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyAgeBRn4RAj"
      },
      "source": [
        "train_copy.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ3GtjbA4Vqg"
      },
      "source": [
        "### 3.2.3 Tokenisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vcb6caTV4ioO"
      },
      "source": [
        "Tokenization is a process of breaking the raw text into small chunks. Tokenization breaks the raw text into words, sentences called tokens. A token is a piece of a whole, so a word is a token in a sentence, and a sentence is a token in a paragraph. Tokenization is the process of splitting a string into a list of tokens. These tokens help in understanding the context or developing the model for the NLP. The tokenization helps in interpreting the meaning of the text by analyzing the sequence of the words. For example, the text “It is raining” can be tokenized into ‘It’, ‘is’, ‘raining’"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS4OmdeG4jEG"
      },
      "source": [
        "tokeniser = TreebankWordTokenizer()\n",
        "train_copy['tokens'] = train_copy['message'].apply(tokeniser.tokenize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Niqnn0xL5Hvr"
      },
      "source": [
        "train_copy.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFeagrYM5NJi"
      },
      "source": [
        "### 3.2.4 Lemmatisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jSaznU-5Uje"
      },
      "source": [
        "Lemmatization is a technique used to extract the base form of words by removing affixes from them and combining common words. It is the process of transforming words into the dictionary base form. These words are linked together based on their semantic relationships. The linking is dependent on the meanings of the words. In particular, we utilize WordNet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcERO-P65VHJ"
      },
      "source": [
        "def lemmas(words, lemmatizer):\n",
        "    return [lemmatizer.lemmatize(word) for word in words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHvWOuaC5Z7C"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "train_copy['lemma'] = train_copy['tokens'].apply(lemmas, args=(lemmatizer, ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vdcs3ifI5j9-"
      },
      "source": [
        "train_copy.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycqTHfIE54-B"
      },
      "source": [
        "# 4. Text Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnvzoMhUvHeI"
      },
      "source": [
        "## 4.1 Splitting out the X variable from the target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roGRykj4vHed"
      },
      "source": [
        "y = train_copy['sentiment']\n",
        "X = train_copy['message']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG6vPmy7vHe1"
      },
      "source": [
        "## 4.2 Data tranformation with TfidfVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLZORKrtUfvB"
      },
      "source": [
        "The Tfidf will be used to transform our data, Tfidf assigns word frequency scores, these scores try to highlight words of greater interest. The TFIDFVectorizer will tokenize the documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZPfrW4lvHe3"
      },
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "X_vectorized = vectorizer.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgEhJNjzvHe0"
      },
      "source": [
        "# 5. Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttE8B7H8vHe9"
      },
      "source": [
        "## 5.1 Splitting the training data into a training and validation set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIewPJjmMFqS"
      },
      "source": [
        "The training data set is split into training and validation dataset. A validation dataset is a sample of data held back from training the model and is used to give an estimate of model skill while tuning the model’s hyperparameters. The validation dataset is different from the test dataset that is also held back from the training of the model but is instead used to give an unbiased estimate of the skill of the final tuned model when comparing or selecting between final models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJ80p_UhvHe-"
      },
      "source": [
        "X_train,X_val,y_train,y_val = train_test_split(X_vectorized,y,test_size=.3,shuffle=True, stratify=y, random_state=11)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Nq3QeodvHfF"
      },
      "source": [
        "## 5.2 Model Fitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqEMw16GvHfG"
      },
      "source": [
        "### 5.2.1 Random Forest Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K02xfjAIF52Q"
      },
      "source": [
        "Random forests is a supervised learning algorithm. Random forests creates decision trees on randomly selected data samples, gets prediction from each tree and selects the best solution by means of voting. It also provides a pretty good indicator of the feature importance. It is said that the more trees it has, the more robust a forest is. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHmfFg_afcDd"
      },
      "source": [
        "from IPython.display import Image\n",
        "Image('rf.png', width=\"800\" ,height=\"400\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FccD-MQ7vHfH"
      },
      "source": [
        "rfc = RandomForestClassifier(n_estimators=100,random_state=42)\n",
        "rfc.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B61VremgvHfR"
      },
      "source": [
        "### 5.2.2 Logistic Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tQAhA2LDa4J"
      },
      "source": [
        "Logistic Regression is a supervised machine learning classification algorithm that is used to predict the probability of a categorical dependent variable. In logistic regression, the dependent variable is a binary variable that contains data coded as 1 (yes, success, etc.) or 0 (no, failure, etc.). In other words, the logistic regression model predicts P(Y=1) as a function of X. Logistic regression is good for binary classes, but in our case there is more than two classes. One-vs-rest(or OvR) approach will be used to combine the logistic regression models. In the OvR case, a separate logistic regression model has trained for each label that the response variable takes on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjtKP6Wyg49C"
      },
      "source": [
        "from IPython.display import Image\n",
        "Image('logistic.jpg', width=\"800\" ,height=\"300\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDmzkmUbvHfT"
      },
      "source": [
        "lmc = LogisticRegression(multi_class='ovr')\n",
        "lmc.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEDpHO4EvHfb"
      },
      "source": [
        "### 5.2.3 Decision Tree Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9Ur6JAb768U"
      },
      "source": [
        "The decision tree model is a supervised machine learning model classification that is in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79j6FBlEhGJH"
      },
      "source": [
        "from IPython.display import Image\n",
        "Image('decision_tree.png', width=\"800\" ,height=\"300\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zb2iv6ekvHfc"
      },
      "source": [
        "dtc = DecisionTreeClassifier(random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrZdal8wvHfh"
      },
      "source": [
        "dtc.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hASkYZ9vHfs"
      },
      "source": [
        "### 5.2.4 Support vector machine Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Blb5C4C7A61p"
      },
      "source": [
        "Support Vector Machine (SVM) is a supervised machine learning algorithm. It works by drawing a straight line hyperplane between two classes.  The data points that fall on one side of the line will be labeled as one class and, the points that fall on the other side will be labeled as the second.  There are many possible hyperplanes that could be chosen, but the main objective is to find a plane that has the maximum margin, i.e the maximum distance between data points of both classes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH_sqtrQhZB-"
      },
      "source": [
        "from IPython.display import Image\n",
        "Image('svm.png', width=\"600\" ,height=\"400\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouXPKZtavHfu"
      },
      "source": [
        "svc = SVC(kernel='linear')\n",
        "svc.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceQYTuVKvHf1"
      },
      "source": [
        "## 6. Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Tuu7cC9JLcS"
      },
      "source": [
        "The base model will be evaluated using the validation dataset that was kept aside from the training data.  After a test dataset will be used to make predictions. That will help us to understand whether we are overfitting our model or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqYlpsrUvHf3"
      },
      "source": [
        "## 6.1 Model evaluation using validation data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oQ6UrCAOBX-"
      },
      "source": [
        "The training data set was split into a training set and an evaluation set. The evaluation set will be used to evaluate the model before evaluated using the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjrDdctPvHf5"
      },
      "source": [
        "# Random forest Predict\n",
        "rfc_pred = rfc.predict(X_val)\n",
        "# Multi-class Logistic Predict\n",
        "lmc_pred = lmc.predict(X_val)\n",
        "#Decision Tree Predict\n",
        "dtc_pred = dtc.predict(X_val)\n",
        "# Support vector Machine Predict\n",
        "svc_pred = svc.predict(X_val)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re7Ki9-1vHgW"
      },
      "source": [
        "## 6.2 Model evaluation using test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-I9bNlljvHgY"
      },
      "source": [
        "## 6.2.1 Data tranformation with Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMYnQS1BvHgZ"
      },
      "source": [
        "testx = test_copy['message']\n",
        "test_vect = vectorizer.transform(testx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdbWecTxvHge"
      },
      "source": [
        "## 6.2.2 Making predictions on the test set "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lx6SVzDSvHgh"
      },
      "source": [
        "# Random Forest\n",
        "rfc_pred_t = rfc.predict(test_vect)\n",
        "# Multi-class Logistic Predict\n",
        "lmc_pred_t = lmc.predict(test_vect)\n",
        "#Decision Tree Predict\n",
        "dtc_pred_t = dtc.predict(test_vect)\n",
        "# Support vector Machine Predict\n",
        "svc_pred_t = svc.predict(test_vect)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FDg47VjvHhB"
      },
      "source": [
        "# 7. Model Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGgFqYgir4as"
      },
      "source": [
        "The performance of a clssification model is based on the counts of test record corrently and incorrectly predicted by the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_NgE8TvvHhk"
      },
      "source": [
        "## 7.1 Classification Report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx0fQjM9JK1f"
      },
      "source": [
        "The Classification Report gives us more information on where our model is going wrong - looking specifically at the performance caused by Type I & II errors. The following metrics are calculated as part of the classification report.\n",
        "\n",
        "**Precision**\n",
        "\n",
        "When it predicts yes, how often is it correct?\n",
        "$$ Precision = \\frac{TP}{TP \\space + FP} = \\frac{TP}{Total \\space Predicted \\space Positive} $$\n",
        "\n",
        "**Recall**\n",
        "\n",
        "When the outcome is actually _yes_, how often do we predict it as such?\n",
        "\n",
        "$$ Recall = \\frac{TP}{TP \\space + FN} = \\frac{TP}{Total \\space Actual \\space Positive}$$\n",
        "\n",
        "**F1 Score**\n",
        "\n",
        "Weighted average of precision and recall. \n",
        "\n",
        "$$F_1 = 2 \\times \\frac {Precision \\space \\times \\space Recall }{Precision \\space + \\space Recall }$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3HSRmkUvHhl"
      },
      "source": [
        "### 7.1.1 Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhHppCmZvHhn",
        "scrolled": true
      },
      "source": [
        "print(\"Classification Report for Validation Dataset\")\n",
        "print(classification_report(y_val, rfc_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJyDcxZVvHhv"
      },
      "source": [
        "### 7.1.2 Logistic Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2y1KjFJgvHhv"
      },
      "source": [
        "print(classification_report(y_val, lmc_pred, target_names=['Anti', 'Nuetral','Pro','News']))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_wPgE2DvHhy"
      },
      "source": [
        "### 7.1.3 Decision Tree Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPbv99hnvHhz"
      },
      "source": [
        "print(classification_report(y_val, dtc_pred, target_names=['Anti', 'Nuetral','Pro','News']))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq06nLGlvHh2"
      },
      "source": [
        "### 7.1.4 Support vector machine Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUv1FN-wvHh3"
      },
      "source": [
        "print(classification_report(y_val, svc_pred, target_names=['Anti', 'Nuetral','Pro','News']))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofGMoz-5vHh-"
      },
      "source": [
        "## 7.2 Overall f1-score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5DriMJEvHh_"
      },
      "source": [
        "# Random Forest\n",
        "rfc_f1=f1_score(y_val, rfc_pred, average=\"macro\")\n",
        "# Logistic Model\n",
        "lmc_f1=f1_score(y_val, lmc_pred, average=\"macro\")\n",
        "#Decision Tree\n",
        "dtc_f1=f1_score(y_val, dtc_pred, average=\"macro\")\n",
        "#Support Vector Machine\n",
        "svc_f1=f1_score(y_val, svc_pred, average=\"macro\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCOBmMN7vHiX"
      },
      "source": [
        "# 8. Submitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-X5dGTwKvHiX"
      },
      "source": [
        "Adding a sentiment column to our original test df"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZR3flO_vHiY"
      },
      "source": [
        "test['sentiment'] = svc_pred_t\n",
        "test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TaSjnWIvHie"
      },
      "source": [
        "Creating an output csv for submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioB67GZfvHif"
      },
      "source": [
        "test[['tweetid','sentiment']].to_csv('testsubmission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIKAWnBVLsE-"
      },
      "source": [
        "The base models did not perform so well in the Kaggle leaderboard and, that is because they were all using default hyperparameters.  In the next following sections, we will look at ways to improve our models by tunning them. Model tuning allows you to customize your models so that they can generate the most accurate outcomes and give you highly valuable insights into your data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyP6WIOfB7do"
      },
      "source": [
        "# Tuned and Improved Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qkdIitoDxgb"
      },
      "source": [
        "# 9. Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh6RlpP-TOWY"
      },
      "source": [
        "In an attempt to improve the machine models, we will start from scratch with the data preprocessing as it might optimize also the process of data cleaning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzK1uItNwwPf"
      },
      "source": [
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore')\n",
        "\n",
        "# Install Prerequisites\n",
        "# import sys\n",
        "# import nltk\n",
        "# !{sys.executable} -m pip install bs4 lxml wordcloud scikit-learn scikit-plot\n",
        "# nltk.download('vader_lexicon')\n",
        "\n",
        "# Exploratory Data Analysis\n",
        "import re\n",
        "import ast\n",
        "import time\n",
        "import nltk\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "#from textblob import TextBlob\n",
        "import matplotlib.pyplot as plt\n",
        "#from wordcloud import WordCloud\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Data Preprocessing\n",
        "import string\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import Counter\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.utils import resample\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import TweetTokenizer \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Classification Models\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Performance Evaluation\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "#from scikitplot.metrics import plot_roc, plot_confusion_matrix\n",
        "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "\n",
        "# Display\n",
        "%matplotlib inline\n",
        "sns.set(font_scale=1)\n",
        "sns.set_style(\"white\")\n",
        "from sklearn.metrics import plot_roc_curve"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfPLvs4FD2o3"
      },
      "source": [
        "#train_data = pd.read_csv('train.csv')\n",
        "#test_data = pd.read_csv('test.csv')\n",
        "#train_data = pd.read_csv('/train.csv')\n",
        "#test_data = pd.read_csv('/test.csv')\n",
        "train_data = train.copy() #For EDA on raw data\n",
        "test_data = test.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAuvtu3tEAtF"
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyGemlKVEDmx"
      },
      "source": [
        "test_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c96zi1fxdfh5"
      },
      "source": [
        "# Final Cleaning\n",
        "def sentiment_changer(df):\n",
        "    \"\"\"\n",
        "    Change key words to reflect the general sentiment associated with it.\n",
        "    \"\"\"\n",
        "    df['message'] = df['message'].apply(lambda x: x.replace('global', 'negative'))\n",
        "    df['message'] = df['message'].apply(lambda x: x.replace('climate', 'positive'))\n",
        "    df['message'] = df['message'].apply(lambda x: x.replace('MAGA', 'negative'))\n",
        "    return df['message']\n",
        "\n",
        "train_data['message'] = sentiment_changer(train_data)\n",
        "test_data['message'] = sentiment_changer(test_data)\n",
        "\n",
        "def clean(df):\n",
        "    \"\"\"\n",
        "    Apply data cleaning steps to raw data.\n",
        "    \"\"\"\n",
        "    df['token'] = df['message'].apply(TweetTokenizer().tokenize) ## first we tokenize\n",
        "    df['punc'] = df['token'].apply(lambda x : [i for i in x if i not in string.punctuation])## remove punctuations\n",
        "    df['dig'] = df['punc'].apply(lambda x: [i for i in x if i not in list(string.digits)]) ## remove digits\n",
        "    df['final'] = df['dig'].apply(lambda x: [i for i in x if len(i) > 1]) ## remove all words with only 1 character\n",
        "    return df['final']\n",
        "\n",
        "train_data['final'] = clean(train_data)\n",
        "test_data['final'] = clean(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-CZftK9ol5L"
      },
      "source": [
        "### Resampling\n",
        "We addressed the problem of imbalanced training data by resampling the data before building our models. A class size was determined based on the second largest sentiment class and other classes were either upsampled or downsampled according to the class size. However, resampling the data did not improve the performance of the models and we therefore excluded it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDzXmd6gdrox"
      },
      "source": [
        "## Lemmatisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z1HRG6Ld1ne"
      },
      "source": [
        "Lemmatisation aims to remove inflectional word endings to return the base or dictionary form of a word, also known as \"lemma\". We used the WordNetLemmatizer() from nltk, as well as by way of applying part of speech."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUmU65EhzRjR"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhUBMLt5d5m8"
      },
      "source": [
        "def get_part_of_speech(word):\n",
        "    \"\"\"\n",
        "    Find part of speech of word if part of speech is either noun, verb, adjective etc and add it to a list.\n",
        "    \"\"\"\n",
        "    probable_part_of_speech = wordnet.synsets(word) ## finding word that is most similar (synonyms) for semantic reasoning\n",
        "    pos_counts = Counter() ## instantiating our counter class\n",
        "    pos_counts[\"n\"] = len([i for i in probable_part_of_speech if i.pos()==\"n\"])\n",
        "    pos_counts[\"v\"] = len([i for i in probable_part_of_speech if i.pos()==\"v\"])\n",
        "    pos_counts[\"a\"] = len([i for i in probable_part_of_speech if i.pos()==\"a\"])\n",
        "    pos_counts[\"r\"] = len([i for i in probable_part_of_speech if i.pos()==\"r\"])\n",
        "    most_likely_part_of_speech = pos_counts.most_common(1)[0][0] ## will extract the most likely part of speech from the list\n",
        "    return most_likely_part_of_speech\n",
        "\n",
        "normalizer = WordNetLemmatizer()\n",
        "\n",
        "train_data['final'] = train_data['final'].apply(lambda x: [normalizer.lemmatize(token, get_part_of_speech(token)) for token in x])\n",
        "test_data['final'] = test_data['final'].apply(lambda x: [normalizer.lemmatize(token, get_part_of_speech(token)) for token in x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkdnFNw8eC3J"
      },
      "source": [
        "## Split Training and Validation Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NL2fv6ueade"
      },
      "source": [
        "Training data: Data that contains a known label. The model is trained on this data to be able to generalize unlabeled data.\n",
        "Validation data: A subset of the training data that is used to assess how well the algorithm was trained on the training data.\n",
        "Test data: Data that is used to provide an unbiased evaluation of the final model fit on the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIuqb_1qegI-"
      },
      "source": [
        "X = train_data['final']\n",
        "y = train_data['sentiment']\n",
        "X_test = test_data['final']\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2usUbQ6eogs"
      },
      "source": [
        "# 10. Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_Xd_wD4euEO"
      },
      "source": [
        "The TfidfVectorizer transforms text to feature vectors that can be used as input to a classification model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ff5RX9HGevi2"
      },
      "source": [
        "X_train = list(X_train.apply(' '.join))\n",
        "X_val = list(X_val.apply(' '.join))\n",
        "\n",
        "vectorizer = TfidfVectorizer(sublinear_tf=True, smooth_idf = True, max_df = 0.3, min_df = 5, ngram_range = (1, 2))\n",
        "vectorizer.fit(X_train)\n",
        "\n",
        "# vect_save_path = \"TfidfVectorizer.pkl\"\n",
        "# with open(vect_save_path,'wb') as file:\n",
        "#     pickle.dump(vectorizer,file)\n",
        "\n",
        "X_train = vectorizer.transform(X_train)\n",
        "X_val = vectorizer.transform(X_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hplRCEiWfGDs"
      },
      "source": [
        "# 11. Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2teDtt2nfQza"
      },
      "source": [
        "## logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoRooQwbfX5f"
      },
      "source": [
        "Logistic regression is a statistical model that makes use of a logistic function to model a binary dependent variable, however, multiclass classification with logistic regression can be done through the one-vs-rest scheme in which a separate model is trained for each class to predict whether an observation is that class or not (thus making it a binary classification problem)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwymi68TfZ3X"
      },
      "source": [
        "modelstart = time.time()\n",
        "logreg = LogisticRegression(C=1000, multi_class='ovr', solver='saga', random_state=42, max_iter=10)\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred = logreg.predict(X_val)\n",
        "logreg_f1 = round(f1_score(y_val, y_pred, average='weighted'),2)\n",
        "logreg_precision = round(precision_score(y_val, y_pred, average='weighted'),4)\n",
        "logreg_recall = round(recall_score(y_val, y_pred, average='weighted'),4)\n",
        "print('Accuracy %s' % accuracy_score(y_pred, y_val))\n",
        "print(\"Model Runtime: %0.2f seconds\"%((time.time() - modelstart)))\n",
        "report = classification_report(y_val, y_pred, output_dict=True)\n",
        "results = pd.DataFrame(report).transpose()\n",
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSi6Jnw5fuoI"
      },
      "source": [
        "## Multinomial Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lBUU_G0fw6H"
      },
      "source": [
        "The Multinomial Naive Bayes model estimates the conditional probability of a particular feature given a class and uses a multinomial distribution for each of the features. The model assumes that each feature makes an independent and equal contribution to the outcome."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMOsu_LBf2Um"
      },
      "source": [
        "modelstart= time.time()\n",
        "multinb = MultinomialNB()\n",
        "multinb.fit(X_train, y_train)\n",
        "y_pred = multinb.predict(X_val)\n",
        "multinb_f1 = round(f1_score(y_val, y_pred, average='weighted'),2)\n",
        "multinb_precision = round(precision_score(y_val, y_pred, average='weighted'),4)\n",
        "multinb_recall = round(recall_score(y_val, y_pred, average='weighted'),4)\n",
        "print('Accuracy %s' % accuracy_score(y_pred, y_val))\n",
        "print(\"Model Runtime: %0.2f seconds\"%((time.time() - modelstart)))\n",
        "report = classification_report(y_val, y_pred, output_dict=True)\n",
        "results = pd.DataFrame(report).transpose()\n",
        "# results.to_csv(\"multinb_report.csv\")\n",
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RN8A-uVf9eP"
      },
      "source": [
        "## Random Forest Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9aNsiHLgCWn"
      },
      "source": [
        "Random forest models are an example of an ensemble method that is built on decision trees (i.e. it relies on aggregating the results of an ensemble of decision trees). Decision tree machine learning models represent data by partitioning it into different sections based on questions asked of independent variables in the data. Training data is placed at the root node and is then partitioned into smaller subsets which form the 'branches' of the tree. In random forest models, the trees are randomized and the model returns the mean prediction of all the individual trees."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b8VTwA8gFpZ"
      },
      "source": [
        "modelstart = time.time()\n",
        "rf = RandomForestClassifier(max_features=4, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred = rf.predict(X_val)\n",
        "rf_f1 = round(f1_score(y_val, y_pred, average='weighted'),2)\n",
        "rf_precision = round(precision_score(y_val, y_pred, average='weighted'),4)\n",
        "rf_recall = round(recall_score(y_val, y_pred, average='weighted'),4)\n",
        "print('Accuracy %s' % accuracy_score(y_pred, y_val))\n",
        "print(\"Model Runtime: %0.2f seconds\"%((time.time() - modelstart)))\n",
        "report = classification_report(y_val, y_pred, output_dict=True)\n",
        "pd.DataFrame(report).transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYYmLiSGgKt0"
      },
      "source": [
        "## Support Vector Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBCxJekVgMiK"
      },
      "source": [
        "A Support Vector Classifier is a discriminative classifier formally defined by a separating hyperplane. When labelled training data is passed to the model, also known as supervised learning, the algorithm outputs an optimal hyperplane which categorizes new data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LAv7hi8gSAG"
      },
      "source": [
        "modelstart = time.time()\n",
        "svc = SVC(gamma = 0.8, C = 10, random_state=42)\n",
        "svc.fit(X_train, y_train)\n",
        "y_pred = svc.predict(X_val)\n",
        "svc_f1 = round(f1_score(y_val, y_pred, average='weighted'),2)\n",
        "svc_precision = round(precision_score(y_val, y_pred, average='weighted'),4)\n",
        "svc_recall = round(recall_score(y_val, y_pred, average='weighted'),4)\n",
        "print('Accuracy %s' % accuracy_score(y_pred, y_val))\n",
        "print(\"Model Runtime: %0.2f seconds\"%((time.time() - modelstart)))\n",
        "report = classification_report(y_val, y_pred, output_dict=True)\n",
        "results = pd.DataFrame(report).transpose()\n",
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExVEAJkN7URj"
      },
      "source": [
        "name = 'svm.pkl'\n",
        "\n",
        "with open (name, 'wb') as file:\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMr9VW8LgYRU"
      },
      "source": [
        "## Linear SVC\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF3fzv14gZ6J"
      },
      "source": [
        "The objective of a Linear Support Vector Classifier is to return a \"best fit\" hyperplane that categorises the data. It is similar to SVC with the kernel parameter set to ’linear’, but it is implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and can scale better to large numbers of samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rU2kpp0gfCu"
      },
      "source": [
        "modelstart = time.time() \n",
        "linsvc = LinearSVC()\n",
        "linsvc.fit(X_train, y_train)\n",
        "y_pred = linsvc.predict(X_val)\n",
        "linsvc_f1 = round(f1_score(y_val, y_pred, average='weighted'),2)\n",
        "linsvc_precision = round(precision_score(y_val, y_pred, average='weighted'),4)\n",
        "linsvc_recall = round(recall_score(y_val, y_pred, average='weighted'),4)\n",
        "print('Accuracy %s' % accuracy_score(y_pred, y_val))\n",
        "print(\"Model Runtime: %0.2f seconds\"%((time.time() - modelstart)))\n",
        "report = classification_report(y_val, y_pred, output_dict=True)\n",
        "results = pd.DataFrame(report).transpose()\n",
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAaqzjLUgn2M"
      },
      "source": [
        "## K Neighbours Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFhd_3q-grf4"
      },
      "source": [
        "The K Neighbours Classifier is a classifier that implements the k-nearest neighbours vote. In classification, the output is a class membership. An object is classified by a plurality vote of its neighbours, with the object being assigned to the class most common among its k-nearest neighbours."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaVKMEkOgukn"
      },
      "source": [
        "modelstart = time.time()\n",
        "kn = KNeighborsClassifier(n_neighbors=1)\n",
        "kn.fit(X_train, y_train)\n",
        "y_pred = kn.predict(X_val)\n",
        "kn_f1 = round(f1_score(y_val, y_pred, average='weighted'),2)\n",
        "kn_precision = round(precision_score(y_val, y_pred, average='weighted'),4)\n",
        "kn_recall = round(recall_score(y_val, y_pred, average='weighted'),4)\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_val))\n",
        "print(\"Model Runtime: %0.2f seconds\"%((time.time() - modelstart)))\n",
        "report = classification_report(y_val, y_pred, output_dict=True)\n",
        "results = pd.DataFrame(report).transpose()\n",
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjCY_NmFg2L-"
      },
      "source": [
        "## Decision Tree Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u7Qd59Gg4dG"
      },
      "source": [
        "Decision tree machine learning models represent data by partitioning it into different sections based on questions asked of independent variables in the data. Training data is placed at the root node and is then partitioned into smaller subsets which form the 'branches' of the tree."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cC6nkRTg9Gf"
      },
      "source": [
        "modelstart = time.time()\n",
        "dt = DecisionTreeClassifier(random_state=42)    \n",
        "dt.fit(X_train, y_train)\n",
        "y_pred = dt.predict(X_val)\n",
        "dt_f1 = round(f1_score(y_val, y_pred, average='weighted'),2)\n",
        "dt_precision = round(precision_score(y_val, y_pred, average='weighted'),4)\n",
        "dt_recall = round(recall_score(y_val, y_pred, average='weighted'),4)\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_val))\n",
        "print(\"Model Runtime: %0.2f seconds\"%((time.time() - modelstart)))\n",
        "report = classification_report(y_val, y_pred, output_dict=True)\n",
        "pd.DataFrame(report).transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9fQUv4ahDrO"
      },
      "source": [
        "## AdaBoost Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZBBJWOFhH9P"
      },
      "source": [
        "The AdaBoost classifier is an iterative ensemble method that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset. In the second step, the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJNqVnvRhNV-"
      },
      "source": [
        "modelstart = time.time()\n",
        "ad = AdaBoostClassifier(random_state=42)\n",
        "ad.fit(X_train, y_train)\n",
        "y_pred = ad.predict(X_val)\n",
        "ad_f1 = round(f1_score(y_val, y_pred, average='weighted'),2)\n",
        "ad_precision = round(precision_score(y_val, y_pred, average='weighted'),4)\n",
        "ad_recall = round(recall_score(y_val, y_pred, average='weighted'),4)\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_val))\n",
        "print(\"Model Runtime: %0.2f seconds\"%((time.time() - modelstart)))\n",
        "report = classification_report(y_val, y_pred, output_dict=True)\n",
        "pd.DataFrame(report).transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5eyGSOvhOEC"
      },
      "source": [
        "## 12. Model Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFTq5GV1hXDN"
      },
      "source": [
        "### Performance Metrics of Best Models\n",
        "\n",
        "We built and tested eight different classification models and compared their performance using a statistical measure known as the weighted F1 score, which takes into account the proportions of each class fed into the model. This is a weighted average of the precision and recall of the model and is the measure that will be used to test the accuracy of our Kaggle output. \n",
        "\n",
        "#### Precision\n",
        "\n",
        "When it predicts \"True\", how often is it correct? \n",
        "\n",
        "$$ Precision = \\frac{TP}{TP \\space + FP} = \\frac{TP}{Total \\space Predicted \\space Positive} $$\n",
        "\n",
        "#### Recall\n",
        "\n",
        "When the outcome is actually \"True\", how often do we predict it as such?\n",
        "\n",
        "$$ Recall = \\frac{TP}{TP \\space + FN} = \\frac{TP}{Total \\space Actual \\space Positive}$$\n",
        "\n",
        "#### F1 Score\n",
        "\n",
        "Weighted average of precision and recall. \n",
        "\n",
        "$$F_1 = 2 \\times \\frac {Precision \\space \\times \\space Recall }{Precision \\space + \\space Recall }$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwEVbWATht8-"
      },
      "source": [
        "# Compare Weighted F1-Scores Between Models\n",
        "fig,axis = plt.subplots(figsize=(10, 5))\n",
        "rmse_x = ['Multinomial Naive Bayes','Logistic Regression','Random Forest Classifier','Support Vector Classifier','Linear SVC','K Neighbours Classifier','Decision Tree Classifier','AdaBoost Classifier']\n",
        "rmse_y = [multinb_f1,logreg_f1,rf_f1,svc_f1,linsvc_f1,kn_f1,dt_f1,ad_f1]\n",
        "ax = sns.barplot(x=rmse_x, y=rmse_y,palette='winter')\n",
        "plt.title('Weighted F1-Score Per Classification Model',fontsize=14)\n",
        "plt.xticks(rotation=90)\n",
        "plt.ylabel('Weighted F1-Score')\n",
        "for p in ax.patches:\n",
        "    ax.text(p.get_x() + p.get_width()/2, p.get_y() + p.get_height(), round(p.get_height(),2), fontsize=12, ha=\"center\", va='bottom')\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG0xJj6Fh1j-"
      },
      "source": [
        "From the performance metrics, we see that the **Support Vector Classifier** performed the best on our validation set, closely followed by the **Linear SVC** and **Logistic Regression** models. The K Neighbours Classifier significantly performed the worst, which may be due to the k value that was selected for the model. To ensure that we get a robust measure of classifier performance, we will apply cross validation and hyperparameter tuning on the top three performing models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixePSAaHh6Bj"
      },
      "source": [
        "## 13. Hyperparameter Tuning of Best Models\n",
        "\n",
        "**Cross validation** is a technique used to test the accuracy of a model's prediction on unseen data (validation sets). This is important because it can assist in picking up issues such as over/underfitting and selection bias. We used the K-fold technique to perform cross validation. \n",
        "\n",
        "**Hyperparameter tuning** is the process by which a set of ideal hyperparameters are chosen for a model. A hyperparameter is a parameter for which the value is set manually and tuned to control the algorithm's learning process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBx_T472h8Xx"
      },
      "source": [
        "**Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxcEahbaiCLi"
      },
      "source": [
        "LogisticRegression().get_params()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Cw3SLAhjrxY"
      },
      "source": [
        "param_grid = {'C': [1000], #[100,1000]\n",
        "              'max_iter': [10], #[10,100]\n",
        "              'multi_class': ['ovr'], #['ovr', 'multinomial']\n",
        "              'random_state': [42],\n",
        "              'solver': ['saga']} #['saga','lbfgs']\n",
        "grid_LR = GridSearchCV(LogisticRegression(), param_grid, scoring='f1_weighted', cv=5, n_jobs=-1)\n",
        "grid_LR.fit(X_train, y_train)\n",
        "y_pred = grid_LR.predict(X_val)\n",
        "print(\"Best parameters:\")\n",
        "print(grid_LR.best_params_)\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_val))\n",
        "print(classification_report(y_val, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5owM-Yz1jtLZ"
      },
      "source": [
        "#### Linear SVC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmvL9eKFjyi7"
      },
      "source": [
        "LinearSVC().get_params()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lWLl6qJkLQV"
      },
      "source": [
        "param_grid = {'C': [100],#[0.1,1,10,100,1000]\n",
        "              'max_iter': [10], #[10,100]\n",
        "              'multi_class' : ['ovr'], #['crammer_singer', 'ovr']\n",
        "              'random_state': [42]} \n",
        "grid_LSVC = GridSearchCV(LinearSVC(), param_grid, scoring='f1_weighted', cv=5, n_jobs=-1)\n",
        "grid_LSVC.fit(X_train, y_train)\n",
        "y_pred = grid_LSVC.predict(X_val)\n",
        "print(grid_LSVC.best_params_)\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_val))\n",
        "print(classification_report(y_val, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnedMaGHkQvm"
      },
      "source": [
        "#### Support Vector Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz5QSZRlkWMx"
      },
      "source": [
        "SVC().get_params()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlQsQ-T8kYyp"
      },
      "source": [
        "param_grid = {'C': [10],#[0.1,1,10,100,1000]\n",
        "              'gamma': [0.8], #[0.8,1]\n",
        "              'kernel': ['rbf'], #['linear','rbf']\n",
        "              'random_state': [42]} \n",
        "grid_SVC = GridSearchCV(SVC(), param_grid, scoring='f1_weighted', cv=5, n_jobs=-1)\n",
        "grid_SVC.fit(X_train, y_train)\n",
        "y_pred = grid_SVC.predict(X_val)\n",
        "print(grid_SVC.best_params_)\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_val))\n",
        "print(classification_report(y_val, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A30tRdLfpVV1"
      },
      "source": [
        "# Random Forest\n",
        "rfc_f1=f1_score(y_val, rfc_pred, average=\"macro\")\n",
        "# Logistic Model\n",
        "lmc_f1=f1_score(y_val, lmc_pred, average=\"macro\")\n",
        "#Decision Tree\n",
        "dtc_f1=f1_score(y_val, dtc_pred, average=\"macro\")\n",
        "#Support Vector Machine\n",
        "svc_f1=f1_score(y_val, svc_pred, average=\"macro\")\n",
        "# AdaBoost Classifier\n",
        "# K Neighbours Classifier\n",
        "#Linear SVC\n",
        "#Multinomial Naive Bayes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kXSq6IAkiBb"
      },
      "source": [
        "# 14. Model Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0sbtAzNks7L"
      },
      "source": [
        "We used a TF-IDF vectorizer to compute a weight for each word token by its level of importance and vectorize it and we used a radial basis function support vector classifier (SVC) to train our model. After a bit of hyperparameter tuning, we found the following parameters to work well: {'C': 10, 'gamma': 0.8, 'kernel': 'rbf', 'random_state': 42}. A token pattern of alphanumeric words performed best and since the average tweet has around 17 words, an n-gram of 1 to 2 performs best in capturing semantic meaning. The SVC parameters were chosen because the radial basis function performs better than a Linear SVC at splitting up the areas in which the different semantic lies. This is possibly due to the fact that the classification is not binary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUxCBCT2kz5G"
      },
      "source": [
        "#### Performance Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN9OyIBdk9Gq"
      },
      "source": [
        "y_pred = svc.predict(X_val)\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_val))\n",
        "print(classification_report(y_val, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHkJ_URUpCl9"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qcW_jxspFer"
      },
      "source": [
        "# Make prediction on test data\n",
        "X = train_data['final']\n",
        "y = train_data['sentiment']\n",
        "X_test = test_data['final']\n",
        "\n",
        "X = list(X.apply(' '.join))\n",
        "X_test = list(X_test.apply(' '.join))\n",
        "\n",
        "vectorizer = TfidfVectorizer(sublinear_tf=True, smooth_idf = True, max_df = 0.3, min_df = 5, ngram_range = (1, 2))\n",
        "vectorizer.fit(X)\n",
        "\n",
        "X = vectorizer.transform(X)\n",
        "X_test = vectorizer.transform(X_test)\n",
        "\n",
        "svc = SVC(gamma=0.8, C=10, random_state=42)\n",
        "svc.fit(X, y)\n",
        "y_test = svc.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKZwfSHyq7OY"
      },
      "source": [
        "# Number of Tweets Per Sentiment Class\n",
        "fig, axis = plt.subplots(ncols=2, figsize=(15, 5))\n",
        "\n",
        "ax = sns.countplot(y_test,palette='winter',ax=axis[0])\n",
        "axis[0].set_title('Number of Tweets Per Sentiment Class',fontsize=14)\n",
        "axis[0].set_xlabel('Sentiment Class')\n",
        "axis[0].set_ylabel('Tweets')\n",
        "for p in ax.patches:\n",
        "    ax.text(p.get_x() + p.get_width()/2., p.get_height(), '%d' % int(p.get_height()), fontsize=11, ha='center', va='bottom')\n",
        "\n",
        "results = pd.DataFrame({\"tweetid\":test_data['tweetid'],\"sentiment\": y_test})\n",
        "results['sentiment'].value_counts().plot.pie(autopct='%1.1f%%',colormap='winter_r',ax=axis[1])\n",
        "axis[1].set_title('Proportion of Tweets Per Sentiment Class',fontsize=14)\n",
        "axis[1].set_ylabel('Sentiment Class')\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXzoixLCrBah"
      },
      "source": [
        "# 15. Save Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGvY_ZS9rJVg"
      },
      "source": [
        "# Create Kaggle Submission File\n",
        "results = pd.DataFrame({\"tweetid\":test_data['tweetid'],\"sentiment\": y_test})\n",
        "results.to_csv(\"Team2_final_submission.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHMx0JznvHi2"
      },
      "source": [
        "# 16. Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bqC6a6LqpMV"
      },
      "source": [
        "In this project, we succeeded in building a supervised machine learning model that is able to classify whether or not a person believes in climate change, based on their novel tweet data. Our top performing model has a weighted F1 score of 0.78, based on our validation set, and the results from our testing set are in line with what was observed in the training set. We think that it is possible that the number of Pro tweets is related to the fact that \"97% or more of actively publishing climate scientists agree: climate-warming trends over the past century are extremely likely due to human activities.\" ([Nasa](https://climate.nasa.gov/scientific-consensus/#*))\n",
        "\n",
        "**Impact investing** is an emerging field that refers to investments made into companies and organisations with the intention to generate measurable social or environmental impact alongside financial return. Many companies are built around lessening one’s environmental impact or carbon footprint and they offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. These companies would like to determine how people perceive climate change and whether or not they believe it is a real threat. Our model provides a valuable solution to this problem and can add to their market research efforts in gauging how their product or service may be received. It gives companies access to a broad base of consumer sentiment, spanning multiple demographic and geographic categories, thus increasing their insights and informing future marketing strategies.\n",
        "\n",
        "From our exploratory data analysis, we can draw some marketing-related insights. For maximum reach in marketing campaigns that target a specific group of people that have a certain stance with regard to climate change, a marketing team can consider the following:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "The rise of impact investment has caused companies to focus on generating a positive social and environmental impact in addition to financial returns. It would assist companies to ally their brand and products with the Pro climate change movement. Pro climate change tweets tend to have a wider reach than other classes. Not only is it an ethical stance but it has potential to increase exposure of the brand on Twitter. Their tweets could be used to add their voice to the fight against global warming and thus be expressed as a negative sentiment or possibly nuetral. This could maximize their reach even further and also introduces other considerations, such as financial rewards due to carbon taxes.\n",
        "\n",
        "\n",
        "Twitter hashtags are a powerful tool that companies can use to reach a larger audience,this can be achieved by engaging in the conversations on climate change by utilising the hashtags that the users that are Pro-Climate change use.This will communicate the company's commitment to man-made climate change.The image of a company plays a vital role in differentiating a company from its competitors as it communicates who it caters to. This minor yet impactful act of using pro-climate change hashtags will improve the image of the business."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNij1MZMvHij"
      },
      "source": [
        "# 17. Comet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxQcptrtvHij"
      },
      "source": [
        "#Create dictionaries for the data we want to log\n",
        "params={'random_state':7,\n",
        "        'model_type':'lmc',\n",
        "        'stratify':True\n",
        "}\n",
        "metrics = {'RFC_F': rf_f1,\n",
        "           'Logreg_F1': logreg_f1,\n",
        "           'DTC_F1':dt_f1,\n",
        "           'SVC_F1':svc_f1,\n",
        "           'Multinb_F1':multinb_f1,\n",
        "           'linsvc_F1':linsvc_f1,\n",
        "           'Kn_precision':kn_f1,\n",
        "           'AdaB_F1':ad_f1}\n",
        "           \n",
        "           # Recall\n",
        "# metrics2={'Logreg_recall':logreg_recall,\n",
        "#            'Multinb_recall':multinb_recall,\n",
        "#            'RFC_recall': rf_recall,\n",
        "#            'SVC_recall':svc_recall,\n",
        "#            'linsvc_recall':linsvc_recall,\n",
        "#            'kn_recall':kn_recall,\n",
        "#            'DTC_recall':dt_recall,\n",
        "#            'AdaB_recall':ad_recall}\n",
        "#            #precisiom\n",
        "# metrics3={ 'Logreg_precision':logreg_precision,\n",
        "#            'Multinb_precision':multinb_precision,\n",
        "#            'RFC_precision':rf_precision,\n",
        "#            'SVC_precision':svc_precision,\n",
        "#            'Linsvc_precision':linsvc_precision,\n",
        "#            'kn_precision':kn_precision,\n",
        "#            'DTC_precision':dt_precision,\n",
        "#            'AdaB_precision':ad_precision\n",
        "# }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gE8mQPVcvHio"
      },
      "source": [
        "experiment.log_parameters(params)\n",
        "experiment.log_metrics(metrics)\n",
        "#experiment.log_metrics(metrics2)\n",
        "#experiment.log_metrics(metrics3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt8VpmOLvHiv"
      },
      "source": [
        "experiment.end()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "087PcbFXvHi3"
      },
      "source": [
        "# 18. References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sNGYJb0CfnW"
      },
      "source": [
        "1. https://towardsdatascience.com/tokenization-for-natural-language-processing-a179a891bad4\n",
        "2. https://towardsdatascience.com/machine-learning-text-processing-1d5a2d638958\n",
        "3. "
      ]
    }
  ]
}